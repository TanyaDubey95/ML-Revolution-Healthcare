{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb359ba5-a4c0-4969-94ce-7a5fed083e53",
   "metadata": {},
   "source": [
    "## ML Revolution in Healthcare: The Diabetes Risk Prediction Challenge.\n",
    " The Diabetes Risk Prediction Challenge Using BRFSS Survey Data.\n",
    " \n",
    "**Problem Statement:**\n",
    "This project addresses the pressing issue of diabetes prevalence in the United States, aiming to predict distinct risk categories using the Behavioral Risk Factor Surveillance System (BRFSS) 2015 dataset. The dataset, comprising 50,000 survey responses, poses a challenge due to class imbalance. The objective is to develop a predictive model that effectively classifies individuals into three categories: 0 for no diabetes/only during pregnancy, 1 for prediabetes, and 2 for diabetes. The emphasis is on mitigating class imbalance to enhance the accuracy of the predictive modeling process.\n",
    "\n",
    "**About the Dataset:**\n",
    "The dataset used in this project is sourced from the CDC's Behavioral Risk Factor Surveillance System (BRFSS) 2015, obtained from the UC Irvine Machine Learning Repository. It comprises 21 feature variables and a target variable (Diabetes_012) categorizing respondents into stages: 0 for no diabetes/only during pregnancy, 1 for prediabetes, and 2 for diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e734e593-ce2e-45f3-9f51-07e8c25dc790",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Intial Exploration:\n",
    "In the initial exploration, the dataset is read, and its shape is checked to understand its structure and dimensions.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd39a72-1d7c-4db7-829a-641d95bc541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = pd.read_csv('./train_dataset.csv')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "dataset.head()\n",
    "\n",
    "# Predict the values with holdout data data. model predictions is a NumPy array or a list\n",
    "X_test_df = pd.read_csv('./holdout.csv')\n",
    "X_test_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7769ec84-6ca3-4d18-90ca-0df9e7a7dc73",
   "metadata": {},
   "source": [
    "### Data Cleaning:\n",
    "Data preprocessing involves cleaning the dataset by handling missing values, removing duplicates, and addressing any inconsistencies or errors. This ensures the data is accurate and reliable for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dda68a-5231-4268-98d3-f10035d7dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = dataset.dropna()\n",
    "# df_selected = dataset\n",
    "df_selected.shape\n",
    "df_selected.groupby(['Diabetes_012']).size()\n",
    "df_selected.head()\n",
    "\n",
    "column_name = 'Diabetes_012'\n",
    "if df_selected[column_name].isna().any():\n",
    "    print(f\"The column '{column_name}' contains NaN values.\")\n",
    "else:\n",
    "    print(f\"The column '{column_name}' does not contain NaN values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59200b21-ed06-4d69-9cca-b9af8835120d",
   "metadata": {},
   "source": [
    "### Transforming Data:\n",
    "This step includes transforming variables or features through methods like scaling, normalization, or encoding categorical variables. Transformation ensures that the data is on a consistent scale and format, preventing certain features from dominating the analysis due to their scale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45cf83b-f5b6-4619-898d-8869744390cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_selected['Diabetes_012']\n",
    "continuous_transformer = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler())  # or StandardScaler()\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('ordinal_encoder', OrdinalEncoder())\n",
    "])\n",
    "continuous_vars = [ 'BMI', 'PhysHlth', 'GenHlth', 'Age', 'MentHlth', 'Education', 'Income']\n",
    "categorical_vars = [\n",
    "    'Sex', 'DiffWalk', 'HighBP', 'HighChol', 'CholCheck', 'Smoker', \n",
    "    'Stroke' , 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', \n",
    "    'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost'\n",
    "]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cont', continuous_transformer, continuous_vars),\n",
    "        ('cat', categorical_transformer, categorical_vars)]\n",
    ")\n",
    "columns = continuous_vars + categorical_vars \n",
    "\n",
    "\n",
    "df_tmp = df_selected[columns]\n",
    "# Apply transformations to the DataFrame\n",
    "preprocessor.fit(df_tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f9d26-291f-45eb-92ab-eb87cc88f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_selected[columns]\n",
    "df_transformed = preprocessor.transform(df_tmp)\n",
    "X_train = pd.DataFrame(df_transformed, columns=columns)\n",
    "\n",
    "# test Data\n",
    "df_test_tmp = X_test_df[columns]\n",
    "df_test_transformed = preprocessor.transform(df_test_tmp)\n",
    "X_test = pd.DataFrame(df_test_transformed, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4199c5a8-f51e-464f-8682-8093e3a895ef",
   "metadata": {},
   "source": [
    "### Model Selection:\n",
    "Choose an appropriate ML model based on the nature of the problem (classification) and the characteristics of the data. Common models include Logistic Regression, Decision Tree, Random Forest, KNN, XGBoost, and AdaBoost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3cb94f-bd99-405e-a61d-0862105a244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X and y are your feature matrix and target variable\n",
    "# Adjust the code based on your actual dataset\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results = pd.DataFrame(columns=['Model', 'Mean F1 Score', 'Std Deviation'])\n",
    "\n",
    "# List of models to evaluate\n",
    "models = [\n",
    "    ('LogisticRegression', LogisticRegression(multi_class='multinomial', solver='lbfgs')),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier()),\n",
    "    ('AdaBoost', AdaBoostClassifier()),\n",
    "    ('KNN', KNeighborsClassifier()),\n",
    "    ('XGBoost', XGBClassifier())\n",
    "]\n",
    "\n",
    "# 5-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate each model and store results based on F1 score\n",
    "for model_name, model in models:\n",
    "    f1_scorer = make_scorer(f1_score, average='macro')  # Adjust 'binary' if you have a multiclass problem\n",
    "    scores = cross_val_score(model, X_train_smote, y_train_smote, scoring=f1_scorer, cv=cv, n_jobs=-1)\n",
    "    mean_f1_score = scores.mean()\n",
    "    std_deviation = scores.std()\n",
    "    model_results = pd.DataFrame({'Model': [model_name], 'Mean F1 Score': [mean_f1_score], 'Std Deviation': [std_deviation]})\n",
    "    results = pd.concat([results, model_results], ignore_index=True)\n",
    "\n",
    "# Display results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de65c9e-f4b2-4648-8f13-f290d840bc77",
   "metadata": {},
   "source": [
    "### Model Training:\n",
    "Train the selected model on the training data. This involves feeding the model the input data and allowing it to learn the patterns within the data.\n",
    "\n",
    "### Hyperparameter Tuning:\n",
    "Fine-tune the model's hyperparameters to optimize its performance. This may involve using techniques like grid search or random search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277fd410-d2b4-4834-b02d-e0fab47d9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {0.0: 48898 / 32599, 1.0: 48898/4427, 2.0: 48898/11872}\n",
    "rf_model = RandomForestClassifier(random_state=42, class_weight=class_weights)\n",
    "# Define the hyperparameter grid to search\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "# Diabetes_012\n",
    "# 0.0    32599\n",
    "# 1.0     4427\n",
    "# 2.0    11872\n",
    "\n",
    "# Perform RandomizedSearchCV for hyperparameter tuning\n",
    "random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist, n_iter=10,\n",
    "                                   scoring='accuracy', cv=5, n_jobs=-1, random_state=42)\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Get the best model from the random search\n",
    "best_rf_model = random_search.best_estimator_\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_rf_model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3916d66b-ae6a-4e1c-903c-6087fb471485",
   "metadata": {},
   "source": [
    "### Predicting:\n",
    "Once the machine learning model is trained and validated, apply it to the holdout data—previously unseen data reserved for testing purposes. This step involves utilizing the trained model to generate predictions or classifications for the holdout dataset, providing insights into how well the model generalizes to new, independent observations.\n",
    "\n",
    "### Post-Prediction Processing:\n",
    "Convert Predicted Values to DataFrame and Export to CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fafe7b-669e-4025-a2ff-85a5e0f7ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = best_rf_model.predict(X_test)\n",
    "\n",
    "# predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# predictions  = best_xgb.predict(X_test)\n",
    "\n",
    "predictions  = classifier.predict(X_test)\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions, columns=['predictions'])\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
